---
title: "HW3"
output: github_document
---

#Load Libraries and Set-Up
```{r setup, include=FALSE}
library(tidyverse)
library(dplyr)
library(viridis)
knitr::opts_chunk$set(echo = TRUE)
```

#Problem 1
```{r}
#Load data from p8105 git repository

library(p8105.datasets)
data("instacart")

##Exploratory analysis to look at generally what products, aisles, and how many of each product are ordered
instacart_df =
  select(instacart, product_name, department, add_to_cart_order, reordered) %>%
  mutate(number_order = add_to_cart_order + reordered) %>%
  arrange(department) %>%
  select(-add_to_cart_order, -reordered)

instacart_df

```

##Aisles Questions
```{r}
#Create a variable that shows the aisles only (smaller dataset)
aisles_df = 
  select(instacart, aisle_id, aisle, product_name, add_to_cart_order, reordered)

#Finding the number of distinct aisles 
nrow(distinct(aisles_df, aisle_id))

#Finding the aisles where the most items are ordered from by grouping them and summarizing number of observations within the aisles column and arranging them in descending order
number_item_df = 
  group_by(aisles_df, aisle) %>%
  summarize(n = n()) %>%
  arrange(desc(n)) %>%
  mutate(number_item = (n)) %>%
  select(-n)

number_item_df
  
```
There are 134 aisles in the dataset with fresh vegetables, fresh fruits, and packaged vegetables fruits aisles containing the most items ordered. The average number of items ordered across all of the aisles is `r mean(pull(number_item_df, number_item))`. The median number of items ordered across all of the aisles is `r median(pull(number_item_df, number_item))`.

##Number of items ordered in each aisle 
```{r}
#Grouped by aisle and summarized by number of observations and then filtered by n > 10000 and renamed the column
ordered_items = 
  group_by(aisles_df, aisle) %>%
  summarize(n = n()) %>%
  arrange(desc(n)) %>%
  filter(n > 10000) %>%
  mutate(number_item = (n)) %>%
  select(-n)

ordered_items

```

## Ggplot of aisles with more than 10,000 items
```{r, fig.width=5}
#Created a ggplot of the items that were ordered more than 10,000 timesm the products are in alphabetical order
ggplot(ordered_items, aes(x = number_item, y = aisle, color = aisle)) +
         geom_point() +
  labs(
    title = "Number of Items Ordered per Aisle",
    x = "Number of Items Ordered",
    y = "Aisle Name"
  )
```

## Table of top three items in Baking Ingredients, Dog Food Care, and Packaged Vegetables Fruits
```{r}
#Filtering the instacart data by baking ingredients, dog food care, and packaged vegetables fruits and pivoting each of them wider, finding the number of times they were ordered, arranging them in descending order, and seeing the top three observations
top_baking_ing = 
  filter(instacart, 
         aisle == "baking ingredients") %>%
  pivot_wider(
         names_from = "aisle",
         values_from = "product_name") %>%
  mutate(number_ordered = 
         add_to_cart_order + reordered) %>%
  arrange(desc(number_ordered)) %>%
  head(3)
  
top_baking_ing

top_dog_food = 
  filter(instacart, 
         aisle == "dog food care") %>%
  pivot_wider(
         names_from = "aisle",
         values_from = "product_name") %>%
  mutate(number_ordered = 
        add_to_cart_order + reordered) %>%
  arrange(desc(number_ordered)) %>%
  head(3)

top_dog_food

top_packaged = 
  filter(instacart, aisle == "packaged vegetables fruits") %>%
    pivot_wider(
      names_from = "aisle",
      values_from = "product_name"
    ) %>%
  mutate(number_ordered = add_to_cart_order + reordered) %>%
  arrange(desc(number_ordered)) %>%
  head(3)
  
top_packaged
```
##Binding the three datasets
```{r}
#Now that I have each of the top three items from the three aisles, I bind the rows together and arrange them in descending order of ordered number

top_three = 
  bind_rows(top_baking_ing, top_dog_food, top_packaged) %>%
  janitor::clean_names() %>%
  select(-everything(), number_ordered, baking_ingredients, dog_food_care, packaged_vegetables_fruits) %>%
  arrange(desc(number_ordered))

top_three
```
The top three items in the baking ingredients aisle are all-purpose flour, no calorie sweetner packets, and light brown sugar. In dog food care it's sausage cuts real beef treats, dentastix small/ medium dog care & treats, and proactive health minichunks adult dog food. In packaged vegetables fruits it's fresh european style baby spinach, organic baby bella mushrooms, and seedless red grapes.

## Mean hour of the day Pink Lady Apples and Coffee Ice Cream
```{r}
## To find the mean hour of the day that pink lady apples and coffee ice cream were ordered I created a new dataframe and filtered by the product name, grouped by product id, recoded week day to be the day of the week, grouped again by product name and week day, mutated to get the mean order hour of teh day, and arranged by day of the week. I used distinct to get only one value for each day of the week.
mean_hour = 
  select(instacart, product_id, product_name, order_hour_of_day, order_dow) %>%
  filter(product_name == "Pink Lady Apples" | product_name == "Coffee Ice Cream") %>%
  group_by(product_id) %>%
  mutate(week_day = recode(order_dow, 
       "0" = "Sunday",
       "1" = "Monday",
       "2" = "Tuesday",
       "3" = "Wednesday",
       "4" = "Thursday",
       "5" = "Friday",
       "6" = "Saturday")) %>%
  group_by(product_name, week_day) %>%
  mutate(mean_hour = mean(order_hour_of_day)) %>%
  select(-order_hour_of_day, -order_dow, -product_id) %>% 
  arrange(week_day) %>%
  distinct() %>%
  knitr::kable()

mean_hour

```


#Problem 2

## Cleaning BRFSS data
```{r}
library(p8105.datasets)
data("BRFSS")

#I downloaded the BRFSS data set, cleaned the names, filtered for the responses I wanted, and arranged the responses from poor to excellent
brfss_data = 
  select(brfss_smart2010, everything()) %>%
  janitor::clean_names() %>%
  filter(topic == "Overall Health" | 
           response == "Excellent" | 
           response == "Very good" | 
           response == "Good" |
           response == "Fair" |
           response == "Poor") %>%
  arrange(response == "Excellent", response == "Very good", response == "Good", response == "Fair", response == "Poor")

brfss_data
  
```
* BRFSS data shows the health status of different locations across states in the US. The responses are recorded as excellent, very good, good, fair, or poor. There are `r nrow(pull(brfss_data))` rows in this dataset. The median response value for the dataset is `r median(pull(brfss_data, data_value))`. 

## 2002:states with 7 or more locations
```{r}
## Created a new dataframe grouped by year and filtered for 2002, grouped by state abbreviation and counted the number of state abbreviations in the dataset, filtered for when the state had more than 7 locations
first_year_data = 
  group_by(brfss_data, year) %>%
  filter(year == 2002) %>%
  group_by(locationabbr) %>%
  count(locationabbr) %>%
  filter(n > 7) %>%
  mutate(number_locations = n) %>%
  select(-n)

first_year_data
```
* The average number of locations a state had in 2002 was `r mean(pull(first_year_data, number_locations))`.
## 2010: states with 7 or more locations
```{r}
# Created a new dataframe grouped by year and filtered for 2010, grouped by state abbreviation and counted the number of state abbreviations in the dataset, filtered for when the state had more than 7 locations
second_year_data = 
  group_by(brfss_data, year) %>%
  filter(year == 2010) %>%
  group_by(locationabbr) %>%
  count(locationabbr) %>%
  filter(n > 7) %>%
  mutate(number_locations = n) %>%
  select(-n)

second_year_data
```
* The average number of locations a state had in 2010 was `r mean(pull(second_year_data, number_locations))`.

##Excellent responses across statements 
```{r}
##Filtered the BRFSS data for excellent responses, grouped by state abbreviation adn year, mutated to find the mean value per location, and grouped by state abbreviation again
excellent_response = 
  select(brfss_data, response, year, locationabbr, data_value) %>%
  filter(response == "Excellent") %>%
  group_by(locationabbr, year) %>%
  mutate(data_location = mean(data_value)) %>%
  select(-data_value) %>%
  group_by(locationabbr) %>%
  distinct()

excellent_response
```

## Spaghetti plot of above dataset
```{r}
# Created a ggplot of the excellent responses across the years by state abbreviations
ggplot(excellent_response, aes(x = year, y = data_location)) + 
  geom_line(aes(
    group = locationabbr, 
    color = locationabbr)) + 
  labs(
    title = 
        "Average Response Between 2002 and 2010 in All States",
    x = "Year",
    y = "Average Response Value") +
  scale_color_viridis(
    name = "locationabbr",
    discrete = TRUE
  ) 
``` 

##Two panel plot showing the states in 2006 and 2010
```{r}
## Filtered the BRFSS data for the state of NY, grouped by locations and response by year. Created a mean of data value by location and filtered for either 2006 or 2010
ny_state = 
  select(brfss_data, response, year, locationabbr, locationdesc, data_value) %>%
  filter(locationabbr == "NY") %>%
  group_by(locationdesc, response, year) %>%
  mutate(data_location = mean(data_value)) %>%
  select(-data_value) %>%
  group_by(locationabbr) %>%
  filter(xor(year == "2006", 
             year == "2010")) %>%
  distinct()

ny_state

```

## Plot of NY in 2006 and 2010
```{r}
#Created a ggplot of NY responses by location between 2006 and 2010. The colors in the bars represent the different locations and the bars themselves are the responses.
ggplot(ny_state, aes(x = response, y = data_location, fill = locationdesc)) +
  geom_col() + 
  labs(
    title = "Response by Location in NY State",
    x = "Response",
    y = "Average Data Value"
  ) + 
  scale_color_viridis(
    name = "locationdesc",
    discrete = TRUE
  ) +
  facet_grid(~year)
  
```


#Problem 3
```{r}
# Read in the accel data, cleaned names, pivoted longer, and grouped by day. Created a new variable for weekend vs weekday and pulled the prefix "activity_" out of the minute variable.
accel_data = 
  read_csv(file = "/Users/hannahbowlin/Documents/Biostats Sem 1/Data Science 1/Data Science/Data Science 1 part a/Visualization and EDA/p8105_hw3_hmb2161/accel_data.csv") %>%
  janitor::clean_names() %>%
  pivot_longer(activity_1:activity_1440,
    names_to = "minute",
    values_to = "activity_amount"
  ) %>%
  group_by(day) %>%
  mutate(type = if_else(day == c("Sat", "Sun"), "weekend", "weekday")) %>%
  mutate(minute = str_replace(minute, "activity_", " "))

accel_data
```
* In this new dataset there are 5 columns instead of over a 1400. There are now `r nrow(pull(accel_data))` rows and `r ncol(pull(accel_data))` columns. The variables are week, day_id, day, minute of the day, and amount of activity in that minute. 

##Total activity per day
```{r}
## I grouped by week and day and then mutated activity amount to be the mean of each day within each week. I then divided mean activity by 60 to give the mean activity in hours. I deselected columns that don't matter and arranged by day of teh week. 
accel_day_data =
  group_by(accel_data, week, day) %>%
  mutate(mean_activity = mean(activity_amount)) %>%
  select(-minute, -activity_amount, -type) %>%
  group_by(day) %>%
  arrange(day) %>%
  distinct()

accel_day_data
  
```
## The trends apparent in the above table are that week 4 seems to have less activity than other weeks. The first week has a lot of activity compared to other weeks. Tuesday's activity is generally less than other days and Wednesday's is more than other days. Tuesday and Wednesday are fairly consistent across weeks in their activity levels. 

```{r}
#Created a ggplot for accel day data where x is the week and y is the mean activity through the weeks. Each line represents a day of the week and reflects how activity changes in one day of the week over the 5 weeks.
ggplot(accel_day_data, aes(x = week, y = mean_activity)) +
  geom_line(aes(group = day, color = day)) +
  labs(
    title = "Average Activity Over Time per Day of the Week",
        x = "Week Number",
        y = "Average Activity Throughout the Day (Minutes)"
  )
```


