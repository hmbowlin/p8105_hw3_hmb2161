---
title: "HW3"
output: github_document
---

#Load Libraries and Set-Up
```{r setup, include=FALSE}
library(tidyverse)
library(dplyr)
library(viridis)
knitr::opts_chunk$set(echo = TRUE)
```

#Problem 1
```{r}
#Load data from p8105 git repository

library(p8105.datasets)
data("instacart")

##Exploratory analysis to look at generally what products, aisles, and how many of each product are ordered
instacart_df =
  select(instacart, product_name, department, add_to_cart_order, reordered) %>%
  mutate(number_order = add_to_cart_order + reordered) %>%
  arrange(department) %>%
  select(-add_to_cart_order, -reordered)

instacart_df

```

##Aisles Questions
```{r}

aisles_df = 
  select(instacart, aisle_id, aisle, product_name, add_to_cart_order, reordered) %>%
  mutate(number_order = add_to_cart_order + reordered)
#Finding the number of distinct aisles 
nrow(distinct(aisles_df, aisle_id))

#Finding the aisles where the most items are ordered from
group_by(aisles_df, aisle) %>%
  summarize(n = n()) %>%
  arrange(desc(n))
  
```
There are 134 aisles in the dataset with fresh vegetables, fresh fruits, and packaged vegetables fruits aisles containing the most items ordered.

##Number of items ordered in each aisle 
```{r}
ordered_items = 
  group_by(aisles_df, aisle) %>%
  summarize(n = n()) %>%
  arrange(desc(n)) %>%
  filter(n > 10000) %>%
  mutate(number_item = (n)) %>%
  select(-n)

ordered_items

```
## Ggplot of aisles with more than 10,000 items
```{r}
ggplot(ordered_items, aes(x = number_item, y = aisle, color = aisle)) +
         geom_point()
```

## Table of top three items in Baking Ingredients, Dog Food Care, and Packaged Vegetables Fruits
```{r}
top_baking_ing = 
  filter(instacart, aisle == "baking ingredients") %>%
    pivot_wider(
      names_from = "aisle",
      values_from = "product_name"
    ) %>%
  mutate(number_ordered = add_to_cart_order + reordered) %>%
  arrange(desc(number_ordered)) %>%
  head(3)
  
top_baking_ing

top_dog_food = 
  filter(instacart, aisle == "dog food care") %>%
    pivot_wider(
      names_from = "aisle",
      values_from = "product_name"
    ) %>%
  mutate(number_ordered = add_to_cart_order + reordered) %>%
  arrange(desc(number_ordered)) %>%
  head(3)

top_dog_food

top_packaged = 
  filter(instacart, aisle == "packaged vegetables fruits") %>%
    pivot_wider(
      names_from = "aisle",
      values_from = "product_name"
    ) %>%
  mutate(number_ordered = add_to_cart_order + reordered) %>%
  arrange(desc(number_ordered)) %>%
  head(3)
  
top_packaged

top_three = 
  bind_rows(top_baking_ing, top_dog_food, top_packaged) %>%
  janitor::clean_names() %>%
  select(-everything(), number_ordered, baking_ingredients, dog_food_care, packaged_vegetables_fruits) %>%
  arrange(desc(number_ordered))

top_three
```

## Mean hour of the day Pink Lady Apples and Coffee Ice Cream
```{r}
mean_hour = 
  select(instacart, product_id, product_name, order_hour_of_day, order_dow) %>%
  filter(product_name == "Pink Lady Apples" | product_name == "Coffee Ice Cream") %>%
  group_by(product_id) %>%
  mutate(week_day = recode(order_dow, 
       "0" = "Sunday",
       "1" = "Monday",
       "2" = "Tuesday",
       "3" = "Wednesday",
       "4" = "Thursday",
       "5" = "Friday",
       "6" = "Saturday")) %>%
  group_by(product_name, week_day) %>%
  mutate(mean_hour = mean(order_hour_of_day)) %>%
  select(-order_hour_of_day, -order_dow, -product_id) %>% 
  arrange(week_day) %>%
  distinct() %>%
  knitr::kable()

mean_hour

```


#Problem 2

## Cleaning BRFSS data
```{r}
library(p8105.datasets)
data("BRFSS")

brfss_data = 
  select(brfss_smart2010, everything()) %>%
  janitor::clean_names() %>%
  filter(topic == "Overall Health" | 
           response == "Excellent" | 
           response == "Very good" | 
           response == "Good" |
           response == "Fair" |
           response == "Poor") %>%
  arrange(response == "Excellent", response == "Very good", response == "Good", response == "Fair", response == "Poor")

brfss_data
  
```

## 2002:states with 7 or more locations
```{r}
first_year_data = 
  group_by(brfss_data, year) %>%
  filter(year == 2002) %>%
  group_by(locationabbr) %>%
  count(locationabbr) %>%
  filter(n > 7) 

first_year_data
```
## 2010: states with 7 or more locations
```{r}
second_year_data = 
  group_by(brfss_data, year) %>%
  filter(year == 2010) %>%
  group_by(locationabbr) %>%
  count(locationabbr) %>%
  filter(n > 7)

second_year_data
```

##Excellent responses across statements 
```{r}
excellent_response = 
  select(brfss_data, response, year, locationabbr, data_value) %>%
  filter(response == "Excellent") %>%
  group_by(locationabbr, year) %>%
  mutate(data_location = mean(data_value)) %>%
  select(-data_value) %>%
  group_by(locationabbr) %>%
  distinct()

excellent_response
```

## Spaghetti plot of above dataset
```{r}
ggplot(excellent_response, aes(x = year, y = data_location)) + 
  geom_line(aes(group = locationabbr, color = locationabbr)) + 
  viridis::scale_color_viridis(
    name = "locationabbr",
    discrete = TRUE
  ) 
``` 

##Two panel plot showing the states in 2006 and 2010
```{r}
ny_state = 
  select(brfss_data, response, year, locationabbr, data_value) %>%
  filter(locationabbr == "NY") %>%
  group_by(locationabbr, response, year) %>%
  mutate(data_location = mean(data_value)) %>%
  select(-data_value) %>%
  group_by(locationabbr) %>%
  filter(xor(year == "2006", year == "2010")) %>%
  distinct()

ny_state

```

minrank!

35 different days on the plots, but you cna plot other trends as well (35 diferent lines)

convert minutes to hour, divide minute variable by 60 
